{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DS 503 Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get started install [Jupyter](http://jupyter.org/) and [Apache Toree](https://github.com/apache/incubator-toree).\n",
    "\n",
    "```shell\n",
    "$ pip install https://dist.apache.org/repos/dist/dev/incubator/toree/0.2.0/snapshots/dev1/toree-pip/toree-0.2.0.dev1.tar.gz\n",
    "$ jupyter toree install\n",
    "```\n",
    "\n",
    "Once you start the notebook with `jupyter notebook`, run the following command (if everything worked, you should see something that looks like `org.apache.spark.SparkContext@5fa635fb`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@494e6780"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Problem 1 SparkSQL (Transaction Data Processing) [30 points]\n",
    "\n",
    "[Guide to SparkSQL commands](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html)\n",
    "\n",
    "Use the Transaction dataset T that you created in Project 1 and create a Spark workflow to do\n",
    "the following. [Use SparkSQL to write this workflow.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------+--------------------+\n",
      "|transaction_id|customer_id|total_amount|item_count|         description|\n",
      "+--------------+-----------+------------+----------+--------------------+\n",
      "|             1|          1|      705.88|         3|F4NYY9eZp0fLoOQEQ...|\n",
      "|             2|          1|    710.3965|         1|bIoRFNFd55cR2rEmh...|\n",
      "|             3|          1|   643.23785|         5|0WPnJAZsOeIlTdE1C...|\n",
      "|             4|          1|    823.4931|         8|XxAqgGnkAI4tvYmBP...|\n",
      "|             5|          1|   768.90283|         3|3mwOIi3HWKL0RvVYs...|\n",
      "|             6|          1|   665.63586|         7|3nzN85bvXybkNpCs0...|\n",
      "|             7|          1|   476.02887|         4|U2rNi2mphaSYCTaLk...|\n",
      "|             8|          1|   630.26965|         8|ip1CFmNLbC5FEv43F...|\n",
      "|             9|          1|   116.05951|         1|cCNufRWg7qVKW1LGp...|\n",
      "|            10|          1|    939.7608|         3|11ACsLbqkjKarSOmO...|\n",
      "|            11|          1|    672.5814|         4|RTSvOg40mg2R7fJTb...|\n",
      "|            12|          1|    454.1324|         5|ZAcDqqSWN1HhOUjrQ...|\n",
      "|            13|          1|    459.1774|         2|9WblccyFZ1OQjHKic...|\n",
      "|            14|          1|   160.99246|         1|qFq5XeGUyhdcfIamV...|\n",
      "|            15|          1|    738.1266|         4|wIkGDs3K4h979cw34...|\n",
      "|            16|          1|   459.06564|        10|KAoxX3v1uiD3Gziv4...|\n",
      "|            17|          1|    798.7524|         9|29AZGr1KuQrknudoYer1|\n",
      "|            18|          1|    579.0614|         8|1Mtm7hJmqWXS52AMohxK|\n",
      "|            19|          1|    771.7162|         5|HQ1FbpG5tczroyOL7...|\n",
      "|            20|          1|    677.1439|        10|uuhffShG83X4AUIuw...|\n",
      "+--------------+-----------+------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val transactionsRDD = sc.textFile(\"data/transactions.csv\")\n",
    "val schemaString = \"transaction_id customer_id total_amount item_count description\"\n",
    "\n",
    "val fields = schemaString.split(\" \").\n",
    "    map(fieldName => StructField(fieldName, StringType, nullable = true))\n",
    "val schema = StructType(fields)\n",
    "\n",
    "val rowRDD = transactionsRDD.\n",
    "    map(_.split(\",\")).\n",
    "    map(attributes => Row(attributes(0), attributes(1), attributes(2), attributes(3), attributes(4).trim))\n",
    "val transactionsDF = spark.createDataFrame(rowRDD, schema)\n",
    "transactionsDF.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "val results = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions\n",
    "    \"\"\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1) T1: Filter out (drop) the transactions from T whose total amount is less than $200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------+--------------------+\n",
      "|transaction_id|customer_id|total_amount|item_count|         description|\n",
      "+--------------+-----------+------------+----------+--------------------+\n",
      "|             1|          1|      705.88|         3|F4NYY9eZp0fLoOQEQ...|\n",
      "|             2|          1|    710.3965|         1|bIoRFNFd55cR2rEmh...|\n",
      "|             3|          1|   643.23785|         5|0WPnJAZsOeIlTdE1C...|\n",
      "|             4|          1|    823.4931|         8|XxAqgGnkAI4tvYmBP...|\n",
      "|             5|          1|   768.90283|         3|3mwOIi3HWKL0RvVYs...|\n",
      "|             6|          1|   665.63586|         7|3nzN85bvXybkNpCs0...|\n",
      "|             7|          1|   476.02887|         4|U2rNi2mphaSYCTaLk...|\n",
      "|             8|          1|   630.26965|         8|ip1CFmNLbC5FEv43F...|\n",
      "|            10|          1|    939.7608|         3|11ACsLbqkjKarSOmO...|\n",
      "|            11|          1|    672.5814|         4|RTSvOg40mg2R7fJTb...|\n",
      "|            12|          1|    454.1324|         5|ZAcDqqSWN1HhOUjrQ...|\n",
      "|            13|          1|    459.1774|         2|9WblccyFZ1OQjHKic...|\n",
      "|            15|          1|    738.1266|         4|wIkGDs3K4h979cw34...|\n",
      "|            16|          1|   459.06564|        10|KAoxX3v1uiD3Gziv4...|\n",
      "|            17|          1|    798.7524|         9|29AZGr1KuQrknudoYer1|\n",
      "|            18|          1|    579.0614|         8|1Mtm7hJmqWXS52AMohxK|\n",
      "|            19|          1|    771.7162|         5|HQ1FbpG5tczroyOL7...|\n",
      "|            20|          1|    677.1439|        10|uuhffShG83X4AUIuw...|\n",
      "|            21|          1|     916.906|         1|0hWBshEkMsUnnlaJl...|\n",
      "|            22|          1|   312.02075|         6|liJ9fcZD3VtJM3jNV...|\n",
      "+--------------+-----------+------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val T1 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions\n",
    "    WHERE total_amount > 200\n",
    "    \"\"\")\n",
    "transactionsDF.registerTempTable(\"T1\")\n",
    "T1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2) T2: Over T1, group the transactions by the Number of Items it has, and for each group calculate the sum of total amounts, the average of total amounts, the min and the max of the total amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var T2 = spark.sql(\"\"\"\n",
    "    SELECT item_count, sum(total_amount) as sum, avg(total_amount) as avg, min(total_amount) as min, max(total_amount) as max\n",
    "    FROM T1\n",
    "    GROUP BY item_count\n",
    "    \"\"\")\n",
    "transactionsDF.registerTempTable(\"T2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3) Report back T2 to the client side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+----------+---------+\n",
      "|item_count|                 sum|              avg|       min|      max|\n",
      "+----------+--------------------+-----------------+----------+---------+\n",
      "|         7|1.0032024631134991E8|550.2668300021387| 100.00456| 999.9923|\n",
      "|         3| 9.986017844713731E7|549.9211324805183| 100.00295|999.99896|\n",
      "|         8|1.0004597069404551E8|549.2323650829262| 100.00547|999.99615|\n",
      "|         5| 9.979772892034207E7|550.7661724761978|100.000854| 999.9972|\n",
      "|         6|1.0025006083922471E8|549.6949172536914| 100.00928|999.99536|\n",
      "|         9| 9.990064196736786E7|550.5957416866522| 100.00365| 999.9996|\n",
      "|         1|1.0018220245438981E8| 550.276299060683| 100.00193| 999.9967|\n",
      "|        10|1.0022187393882251E8|549.7966083462113| 100.00204|999.99243|\n",
      "|         4| 9.958202809550999E7|548.3380491694152| 100.00327|999.97284|\n",
      "|         2|1.0031883233221298E8|550.0961381628866| 100.00756| 999.9979|\n",
      "+----------+--------------------+-----------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4) T3: Over T1, group the transactions by customer ID, and for each group report the customer ID, and the transactions’ count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|customer_id|transaction_count|\n",
      "+-----------+-----------------+\n",
      "|        296|              102|\n",
      "|        467|              102|\n",
      "|        675|              102|\n",
      "|        691|              102|\n",
      "|        829|              102|\n",
      "|       1090|              102|\n",
      "|       1159|              102|\n",
      "|       1436|              102|\n",
      "|       1512|              102|\n",
      "|       1572|              102|\n",
      "|       2069|              102|\n",
      "|       2088|              102|\n",
      "|       2136|              102|\n",
      "|       2162|              102|\n",
      "|       2294|              102|\n",
      "|       2904|              102|\n",
      "|       3210|              102|\n",
      "|       3414|              102|\n",
      "|       3606|              102|\n",
      "|       3959|              102|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var T3 = spark.sql(\"\"\"\n",
    "    SELECT customer_id, count(*) as transaction_count\n",
    "    FROM T1\n",
    "    GROUP BY customer_id\n",
    "    \"\"\")\n",
    "T3.registerTempTable(\"T3\")\n",
    "T3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[customer_id: string, transaction_count: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5) T4: Filter out (drop) the transactions from T whose total amount is less than $600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------+--------------------+\n",
      "|transaction_id|customer_id|total_amount|item_count|         description|\n",
      "+--------------+-----------+------------+----------+--------------------+\n",
      "|             7|          1|   476.02887|         4|U2rNi2mphaSYCTaLk...|\n",
      "|             9|          1|   116.05951|         1|cCNufRWg7qVKW1LGp...|\n",
      "|            12|          1|    454.1324|         5|ZAcDqqSWN1HhOUjrQ...|\n",
      "|            13|          1|    459.1774|         2|9WblccyFZ1OQjHKic...|\n",
      "|            14|          1|   160.99246|         1|qFq5XeGUyhdcfIamV...|\n",
      "|            16|          1|   459.06564|        10|KAoxX3v1uiD3Gziv4...|\n",
      "|            18|          1|    579.0614|         8|1Mtm7hJmqWXS52AMohxK|\n",
      "|            22|          1|   312.02075|         6|liJ9fcZD3VtJM3jNV...|\n",
      "|            25|          1|   280.80206|         5|4q9AJH152KKIriNsa...|\n",
      "|            27|          1|    544.7473|         3|1bSnaCqf2b1vpZWYt...|\n",
      "|            28|          1|   327.94125|         6|bA2y4FUJ3GHM8Lllr...|\n",
      "|            29|          1|   543.52203|         1|1KnV9qjo76afvdafDIZX|\n",
      "|            32|          1|   534.67126|         7|DSAUTxcNXg7W5QBX7...|\n",
      "|            33|          1|   347.82996|         8|3UIqqppYQ5wBJmuA3...|\n",
      "|            34|          1|   340.93652|         6|cjjKWVM6aAxMJiuyZ...|\n",
      "|            35|          1|   369.94492|         3|tNkr7OISqvwVsBlSa...|\n",
      "|            37|          1|   326.20517|         4|Ne7iejfV5XwYo6PgUlqu|\n",
      "|            38|          1|    497.7991|         5|7OeiywgYeUESiJXNp...|\n",
      "|            40|          1|   327.44507|         1|002Xsp4ZpC322tggk...|\n",
      "|            42|          1|   166.89853|         3|LWOAuLXoKdaQPib9w...|\n",
      "+--------------+-----------+------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var T4 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM transactions\n",
    "    WHERE total_amount < 600\n",
    "    \"\"\")\n",
    "transactionsDF.registerTempTable(\"T4\")\n",
    "T4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6) T5: Over T4, group the transactions by customer ID, and for each group report the customer ID, and the transactions’ count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-----------+-----------------+\n",
      "|customer_id|transaction_count|\n",
      "+-----------+-----------------+\n",
      "|        296|              102|\n",
      "|        467|              102|\n",
      "|        675|              102|\n",
      "|        691|              102|\n",
      "|        829|              102|\n",
      "|       1090|              102|\n",
      "|       1159|              102|\n",
      "|       1436|              102|\n",
      "|       1512|              102|\n",
      "|       1572|              102|\n",
      "|       2069|              102|\n",
      "|       2088|              102|\n",
      "|       2136|              102|\n",
      "|       2162|              102|\n",
      "|       2294|              102|\n",
      "|       2904|              102|\n",
      "|       3210|              102|\n",
      "|       3414|              102|\n",
      "|       3606|              102|\n",
      "|       3959|              102|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var T5 = spark.sql(\"\"\"\n",
    "    SELECT customer_id, count(*) as transaction_count\n",
    "    FROM T4\n",
    "    GROUP BY customer_id \n",
    "    \"\"\")\n",
    "transactionsDF.registerTempTable(\"T5\")\n",
    "T5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 7) T6: Select the customer IDs whose T5.count * 3 < T3.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var T6 = spark.sql(\"\"\"\n",
    "    SELECT T5.customer_id\n",
    "    FROM T5\n",
    "    JOIN T3\n",
    "    ON T5.customer_id = T3.customer_id\n",
    "    WHERE transaction_count * 3 < transaction_count\n",
    "    GROUP BY T5.customer_id\n",
    "    \"\"\")\n",
    "transactionsDF.registerTempTable(\"T6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 8) Report back T6 to the client side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Problem 2 Spark-RDDs (Grid Cells of High Relative-Density Index) [70 points]\n",
    "\n",
    "Assume a two-dimensional space that extends from 1...10,000 in each dimension as shown in Figure 1. There are points scattered all around the space. The space is divided into pre-defined grid-cells, each of size 20x20. That is, there is 500,000 grid cell in the space. Each cell has a unique ID as indicated in the Figure. Given an ID of a grid cell, you can calculate the row and the column it belongs to using a simple mathematical equation.\n",
    "\n",
    "### Step 1 (Create the Datasets) [10 Points] //You can re-use your code from Project 2\n",
    "\n",
    "* Your task in this step is to create one dataset P (set of 2D points). Assume the space extends from 1...10,000 in the both the X and Y axis. Each line in the file should contain one point in the format (a, b), where a is the value in the X axis, and b is the value in the Y axis.\n",
    "* Scale the dataset to be at least 100MB.\n",
    "* Choose the appropriate random function (of your choice) to create the points.\n",
    "* Upload and store the file into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val RECORD_COUNT = 10000000 // 97.8MB\n",
    "val LOWER = 1\n",
    "val HIGHER = 10000\n",
    "\n",
    "val writer = new PrintWriter(new File(\"data/points.csv\"))\n",
    "for (_ <- 1 to RECORD_COUNT) {\n",
    "    val x = scala.util.Random.nextInt(HIGHER - LOWER) + LOWER\n",
    "    val y = scala.util.Random.nextInt(HIGHER - LOWER) + LOWER\n",
    "    writer.println(s\"$x,$y\")\n",
    "}\n",
    "writer.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CellCoordinates(val x1: Int, val x2: Int, val y1: Int, val y2: Int) extends Serializable {\n",
    "    override def toString(): String = s\"($x1, $x2, $y1, $y2)\"\n",
    "}\n",
    "\n",
    "def get_cell_coordinates(cell_number: Int, row_length: Int = 500): CellCoordinates = {\n",
    "    val x = (cell_number - 1) % row_length\n",
    "    val y = Math.floor((cell_number - 1) / row_length).toInt\n",
    "\n",
    "    val x1 = x * 20\n",
    "    val x2 = (x + 1) * 20\n",
    "\n",
    "    val y1 = y * 20\n",
    "    val y2 = (y + 1) * 20\n",
    "    return new CellCoordinates(x1, x2, y1, y2)\n",
    "}\n",
    "\n",
    "def is_point_in_cell(point: Array[String], cell: CellCoordinates): Boolean = {\n",
    "    val x = point(0).toInt\n",
    "    val y = point(1).toInt\n",
    "    return (x >= cell.x1) && (x < cell.x2) && (y >= cell.y1) && (y < cell.y2)\n",
    "}\n",
    "\n",
    "def get_cell_neighbors(cell_number: Int, row_length: Int = 500): Array[Int] = {\n",
    "    val top_neighbor_number = cell_number - row_length\n",
    "    val bottom_neighbor_number = cell_number + row_length\n",
    "    var neighbor_numbers = Array[Int](top_neighbor_number, bottom_neighbor_number)\n",
    "    \n",
    "    if ((cell_number - 1) % 500 != 0) {\n",
    "        val left_neighbor_number = cell_number - 1\n",
    "        val top_left_neighbor_number = top_neighbor_number - 1\n",
    "        val bottom_left_neighbor_number = bottom_neighbor_number - 1\n",
    "        neighbor_numbers = neighbor_numbers ++ Array[Int](left_neighbor_number,\n",
    "                                                          top_left_neighbor_number,\n",
    "                                                          bottom_left_neighbor_number)\n",
    "    }\n",
    "    \n",
    "    if (cell_number % 500 != 0) {\n",
    "        val right_neighbor_number = cell_number + 1\n",
    "        val top_right_neighbor_number = top_neighbor_number + 1\n",
    "        val bottom_right_neighbor_number = bottom_neighbor_number + 1\n",
    "        neighbor_numbers = neighbor_numbers ++ Array[Int](right_neighbor_number,\n",
    "                                                          top_right_neighbor_number,\n",
    "                                                          bottom_right_neighbor_number)\n",
    "    }\n",
    "\n",
    "    return neighbor_numbers.filter(number => number > 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val pointsRDD = sc.textFile(\"data/points.csv\").\n",
    "                   map(line => line.split(\",\").map(elem => elem.trim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Report the TOP 50 grid cells w.r.t Relative-Density Index) [40 Points]\n",
    "\n",
    "In this step, you will write Scala or Java code (it is your choice) to manipulate the file and report the top 50 grid cells (the grid cell IDs not the points inside) that have the highest I index. Write the workflow that reports the cell IDs along with their relative-density index.\n",
    "Your code should be fully parallelizable (distributed) and scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 / 42 = 0.9285714\n"
     ]
    }
   ],
   "source": [
    "val cell_number = 502\n",
    "val cell = get_cell_coordinates(cell_number)\n",
    "val cell501RDD = pointsRDD.filter(point => is_point_in_cell(point, cell))\n",
    "val x_count = cell501RDD.count\n",
    "\n",
    "val cell501NeighborNumbers = get_cell_neighbors(cell_number)\n",
    "val y_count = cell501NeighborNumbers.map(number => get_cell_coordinates(number)).\n",
    "                                     map(neighbor => pointsRDD.filter(point => is_point_in_cell(point, neighbor))).\n",
    "                                     map(rdd => rdd.count)\n",
    "val y_count_average = y_count.sum / y_count.length\n",
    "\n",
    "println(s\"$x_count / $y_count_average = ${x_count.toFloat / y_count_average.toFloat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Report the TOP 50 grid cells w.r.t Relative-Density Index)[20 Points]\n",
    "\n",
    "Continue over the results from Step 2, and for each of the reported top 50 grid cells, report the IDs and the relative-density indexes of its neighbor cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
